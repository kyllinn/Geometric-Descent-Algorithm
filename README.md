# Project Description:
The main content of our project is about a geometric gradient descent algorithm——GeoD. GeoD is a new method for unconstrained optimization of a smooth and strongly convex function. As argued and proved in "A geometric alternative to Nesterov’s accelerated gradient descent" (Sébastien Bubeck, Yin Tat Lee, Mohit Singh, 2015), GeoD could attain the optimal rate of convergence of Nesterov’s accelerated gradient descent. One advantage of this geometric algorithm is that it has a much simpler interpretation than Nesterov's accelerated algorithm.

# File Description:
**Writeup.ipynb:** Description and Conclusion of the Project\
**Experiment1.ipynb:** Comparisons of GeoD with other Optimizers\
**Experiment2.ipynb:** Dip into the core algorithm in GeoD: Line Search\
**Loss_and_Optimizer.py:** Loss Function and Optimizer Classes\

# Team Members:
Bingdao Chen\
Keyuan Lin\
Yuxiang Lin\
Zhiwei Wang
